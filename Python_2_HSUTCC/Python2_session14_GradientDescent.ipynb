{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-1.52533979 -0.34456174 -0.26328669]\n",
      " [-0.3866013   1.39683547  0.02544633]\n",
      " [-0.91252505  0.31617937 -3.55318463]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # we do matrix-vec multiplication of x and w and produce n samples\n",
    "    # then we add the bias scalar to every element.\n",
    "    y_prediction = X @ w + b \n",
    "    return y_prediction \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_prediction: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "    # to calculate the error for n_Samples\n",
    "    error = y_prediction - y_true\n",
    "    \n",
    "    # Square errors elementwise (y_pred - y_true)**2\n",
    "    squared_error = error ** 2\n",
    "    \n",
    "    # mean of all squared errors sum / n\n",
    "    MSE = np.mean(squared_error)\n",
    "    \n",
    "    return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_prediction: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Num samp\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # error vec (y-pred - y)\n",
    "    # shape (n_sample)\n",
    "    error = y_prediction - y\n",
    "    \n",
    "    grad_w = (2/ n_samples) * (X.T @ error)\n",
    "    \n",
    "    grad_b = (2 / n_samples) * np.sum(error)\n",
    "    \n",
    "    return grad_w, grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "w = np.ones((3))\n",
    "b = np.array([0.0])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), (1,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b.shape # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.85186442,  9.29304439, -1.0522734 ]), np.float64(-10.19197986236907))\n"
     ]
    }
   ],
   "source": [
    "y_prediction = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_prediction)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01 # (3,)\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = [] # empty list to store loss values\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # calculate y_prediction\n",
    "        y_prediction = predict(X, w, b)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = compute_mse(y, y_prediction)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # calculate gradient\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_prediction)\n",
    "        \n",
    "        # update weights and bias\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.849160\n",
      "Iteration   25 | Loss: 16.061773\n",
      "Iteration   50 | Loss: 5.744300\n",
      "Iteration   75 | Loss: 2.167009\n",
      "Iteration  100 | Loss: 0.925257\n",
      "Iteration  125 | Loss: 0.493654\n",
      "Iteration  150 | Loss: 0.343418\n",
      "Iteration  175 | Loss: 0.291034\n",
      "Iteration  200 | Loss: 0.272734\n",
      "Iteration  225 | Loss: 0.266327\n",
      "Iteration  250 | Loss: 0.264079\n",
      "Iteration  275 | Loss: 0.263288\n",
      "Iteration  300 | Loss: 0.263009\n",
      "Iteration  325 | Loss: 0.262910\n",
      "Iteration  350 | Loss: 0.262875\n",
      "Iteration  375 | Loss: 0.262862\n",
      "Iteration  400 | Loss: 0.262858\n",
      "Iteration  425 | Loss: 0.262856\n",
      "Iteration  450 | Loss: 0.262856\n",
      "Iteration  475 | Loss: 0.262856\n",
      "Iteration  499 | Loss: 0.262855\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00857049 -3.51317235  1.48090465]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9908\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 85.696115\n",
      "Iteration   25 | Loss: 871155159202081111520571956442045941119879752400970050021201808697372616902517776369758662489941122009264930032502356888346629332618898026256512926205772115572748248142770935687381324059140750992520089259333243633664.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theun\\AppData\\Local\\Temp\\ipykernel_40200\\1286522444.py:16: RuntimeWarning: overflow encountered in square\n",
      "  squared_error = error ** 2\n",
      "c:\\Users\\theun\\anaconda3\\Lib\\site-packages\\numpy\\_core\\_methods.py:134: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\theun\\AppData\\Local\\Temp\\ipykernel_40200\\2316166445.py:20: RuntimeWarning: overflow encountered in matmul\n",
      "  grad_w = (2/ n_samples) * (X.T @ error)\n",
      "C:\\Users\\theun\\AppData\\Local\\Temp\\ipykernel_40200\\2316166445.py:20: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad_w = (2/ n_samples) * (X.T @ error)\n",
      "c:\\Users\\theun\\anaconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\theun\\AppData\\Local\\Temp\\ipykernel_40200\\2058285838.py:43: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - learning_rate * grad_w\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    mean_column = np.mean(X, axis=0)\n",
    "    std_column = np.std(X, axis=0)\n",
    "    X_std = (X - mean_column) / std_column\n",
    "    return X_std, mean_column, std_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges:\n",
      "  Feature 1: [-3.31, 3.92]\n",
      "  Feature 2: [-2.79, 2.66]\n",
      "  Feature 3: [-2.98, 2.47]\n",
      "Iteration    0 | Loss: 29.764856\n",
      "Iteration   25 | Loss: 11.033700\n",
      "Iteration   50 | Loss: 4.188505\n",
      "Iteration   75 | Loss: 1.685461\n",
      "Iteration  100 | Loss: 0.769650\n",
      "Iteration  125 | Loss: 0.434379\n",
      "Iteration  150 | Loss: 0.311569\n",
      "Iteration  175 | Loss: 0.266558\n",
      "Iteration  200 | Loss: 0.250052\n",
      "Iteration  225 | Loss: 0.243995\n",
      "Iteration  250 | Loss: 0.241771\n",
      "Iteration  275 | Loss: 0.240954\n",
      "Iteration  300 | Loss: 0.240654\n",
      "Iteration  325 | Loss: 0.240543\n",
      "Iteration  350 | Loss: 0.240502\n",
      "Iteration  375 | Loss: 0.240488\n",
      "Iteration  400 | Loss: 0.240482\n",
      "Iteration  425 | Loss: 0.240480\n",
      "Iteration  450 | Loss: 0.240479\n",
      "Iteration  475 | Loss: 0.240479\n",
      "Iteration  499 | Loss: 0.240479\n"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.01, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    # Use same code as in class.\n",
    "    \n",
    "    for i in range(n_iterations): \n",
    "        # calculate y_prediction\n",
    "        y_prediction = predict(X, w, b)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = compute_mse(y, y_prediction)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # calculate gradient\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_prediction)\n",
    "        \n",
    "        # update weights and bias\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.884575\n",
      "Iteration   50 | Loss: 5.747790\n",
      "Iteration  100 | Loss: 0.925549\n",
      "Iteration  150 | Loss: 0.343432\n",
      "Iteration  199 | Loss: 0.273155\n"
     ]
    }
   ],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    \n",
    "    def get_learning_rate(t: int) -> float:\n",
    "        \"\"\"Return learning rate at iteration t\"\"\"\n",
    "        if schedule == 'step':\n",
    "            # step decay = lr = lr0 * 0.9^(floor(t / 100))\n",
    "            return initial_lr * (0.9 ** (t // 100))\n",
    "        elif schedule == 'exponential':\n",
    "            # expo decay = lr = lr0 * exp(-k * t)\n",
    "            return initial_lr * np.exp(-decay_constant * t)\n",
    "        elif schedule == 'inverse':\n",
    "            # inverse time = lr = lr0 / (1 + k * t)\n",
    "            return initial_lr / (1 + decay_constant * t)\n",
    "        else:\n",
    "            # error handling\n",
    "            raise ValueError(\"Schedule must be step, exponential, or inverse.\")\n",
    "        \n",
    "    for t in range(n_iterations):\n",
    "        # Update LR\n",
    "        learning_rate = get_learning_rate(t)\n",
    "        \n",
    "        # predictions\n",
    "        y_prediction = predict(X, w, b)\n",
    "        \n",
    "        # loss\n",
    "        loss = compute_mse(y, y_prediction)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # gradients\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_prediction)\n",
    "        \n",
    "        # gradients with updated LR\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step decay:\n",
      "final decay:  0.26285721163067477\n",
      "Exponential decay:\n",
      "final decay:  0.9458219532452796\n",
      "Inverse time decay:\n",
      "final decay:  0.2872568429148417\n"
     ]
    }
   ],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.01\n",
    ")\n",
    "print(\"final decay: \", loss_history[-1])\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.01\n",
    ")\n",
    "print(\"final decay: \", loss_history[-1])\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.01\n",
    ")\n",
    "print(\"final decay: \", loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_prediction: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    mse = compute_mse(y_true, y_prediction)\n",
    "    l2_term = reg_lambda * np.sum(w ** 2)\n",
    "    return mse + l2_term\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_prediction: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    grad_w, grad_b = compute_gradients(X, y, y_prediction)\n",
    "    \n",
    "    # add l2 pnealty to grad_w\n",
    "    grad_w += 2 * reg_lambda * w\n",
    "    \n",
    "    return grad_w, grad_b\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    loss_history = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # prediciton\n",
    "        y_prediction = predict(X, w, b)\n",
    "        \n",
    "        # ridge loss\n",
    "        loss = calculate_ridge_loss(y, y_prediction, w, reg_lambda)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # ridge grad\n",
    "        grad_w, grad_b = calculate_ridge_gradients(X, y, y_prediction, w, reg_lambda)\n",
    "        \n",
    "        # update w, b\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "    return w, b, loss_history        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.82622314, -3.20131256,  1.32786329]), np.float64(5.01006630159851), [np.float64(45.77978616181906), np.float64(43.89063029237924), np.float64(42.08320433950511), np.float64(40.35396050297315), np.float64(38.69950546236967), np.float64(37.11659363182946), np.float64(35.6021207100524), np.float64(34.15311751264216), np.float64(32.766744074380625), np.float64(31.4402840095962), np.float64(30.17113911930514), np.float64(28.956824234302132), np.float64(27.794962283852694), np.float64(26.68327958009409), np.float64(25.61960130868697), np.float64(24.60184721667481), np.float64(23.628027488906113), np.float64(22.696238804753893), np.float64(21.804660567230002), np.float64(20.95155129693928), np.float64(20.135245183649957), np.float64(19.354148788574403), np.float64(18.606737890757078), np.float64(17.89155447125706), np.float64(17.207203829089178), np.float64(16.552351823153074), np.float64(15.92572223463277), np.float64(15.326094244591344), np.float64(14.752300021717117), np.float64(14.203222415398868), np.float64(13.677792749519277), np.float64(13.174988712558175), np.float64(12.693832339790465), np.float64(12.233388083548554), np.float64(11.79276096769596), np.float64(11.371094822627615), np.float64(10.967570597274136), np.float64(10.581404744741725), np.float64(10.211847678367011), np.float64(9.85818229510744), np.float64(9.519722563322773), np.float64(9.195812172132257), np.float64(8.885823239655547), np.float64(8.58915507756333), np.float64(8.305233009476447), np.float64(8.033507240860121), np.float64(7.773451778163038), np.float64(7.524563395049556), np.float64(7.286360643667628), np.float64(7.058382908985044), np.float64(6.840189504312883), np.float64(6.631358806217299), np.float64(6.431487427099626), np.float64(6.2401894238000635), np.float64(6.0570955406521465), np.float64(5.881852485484136), np.float64(5.714122237129229), np.float64(5.553581383069397), np.float64(5.399920485897919), np.float64(5.252843477343098), np.float64(5.112067078650753), np.float64(4.9773202461756245), np.float64(4.848343641082132), np.float64(4.724889122103013), np.float64(4.60671926035037), np.float64(4.493606875217548), np.float64(4.385334590452412), np.float64(4.281694409522659), np.float64(4.18248730943236), np.float64(4.087522852185549), np.float64(3.996618813127935), np.float64(3.909600825431329), np.float64(3.826302040017526), np.float64(3.7465628002491336), np.float64(3.6702303307441717), np.float64(3.597158439699386), np.float64(3.527207234134054), np.float64(3.460242847491786), np.float64(3.3961371790623156), np.float64(3.3347676447088332), np.float64(3.2760169384088194), np.float64(3.21977280413783), np.float64(3.1659278176462173), np.float64(3.1143791776984076), np.float64(3.0650285063631193), np.float64(3.017781657960887), np.float64(2.972548536292404), np.float64(2.9292429197876215), np.float64(2.887782294231246), np.float64(2.8480876927353043), np.float64(2.810083542643763), np.float64(2.7736975190679765), np.float64(2.7388604047648197), np.float64(2.7055059560819386), np.float64(2.6735707747065796), np.float64(2.6429941849658887), np.float64(2.6137181164376115), np.float64(2.585686991640589), np.float64(2.5588476185845055), np.float64(2.5331490879679395), np.float64(2.5085426748229565), np.float64(2.4849817444132762), np.float64(2.46242166220143), np.float64(2.4408197077083686), np.float64(2.4201349920966693), np.float64(2.400328379315827), np.float64(2.3813624106551425), np.float64(2.3632012325564586), np.float64(2.3458105275453915), np.float64(2.3291574481458897), np.float64(2.3132105536487972), np.float64(2.2979397496107383), np.float64(2.2833162299650183), np.float64(2.2693124216313745), np.float64(2.2559019315163225), np.float64(2.2430594958005687), np.float64(2.230760931414432), np.float64(2.2189830896065375), np.float64(2.2077038115151657), np.float64(2.196901885655559), np.float64(2.186557007240265), np.float64(2.1766497392531896), np.float64(2.1671614752014947), np.float64(2.158074403472738), np.float64(2.1493714732278297), np.float64(2.141036361763387), np.float64(2.13305344327994), np.float64(2.12540775899522), np.float64(2.1180849885443673), np.float64(2.1110714226114524), np.float64(2.1043539367390895), np.float64(2.097919966265245), np.float64(2.0917574823385374), np.float64(2.0858549689654553), np.float64(2.0802014010449112), np.float64(2.074786223347516), np.float64(2.0695993303987668), np.float64(2.0646310472271443), np.float64(2.0598721109397813), np.float64(2.055313653089992), np.float64(2.050947182802493), np.float64(2.046764570623642), np.float64(2.0427580330653976), np.float64(2.0389201178131087), np.float64(2.035243689568496), np.float64(2.031721916500442), np.float64(2.0283482572773934), np.float64(2.0251164486563114), np.float64(2.022020493604181), np.float64(2.0190546499291404), np.float64(2.0162134193992793), np.float64(2.0134915373280964), np.float64(2.0108839626065276), np.float64(2.0083858681623115), np.float64(2.005992631828304), np.float64(2.0036998276021336), np.float64(2.001503217280362), np.float64(1.9993987424510338), np.float64(1.997382516829198), np.float64(1.995450818920656), np.float64(1.9936000849998075), np.float64(1.9918269023881077), np.float64(1.990128003020198), np.float64(1.9885002572853516), np.float64(1.9869406681324047), np.float64(1.9854463654268464), np.float64(1.9840146005492438), np.float64(1.9826427412246275), np.float64(1.981328266572925), np.float64(1.9800687623709512), np.float64(1.97886191651687), np.float64(1.9777055146884417), np.float64(1.97659743618674), np.float64(1.9755356499573749), np.float64(1.974518210781617), np.float64(1.973543255630127), np.float64(1.9726090001723247), np.float64(1.971713735434719), np.float64(1.9708558246018182), np.float64(1.9700336999535077), np.float64(1.9692458599330458), np.float64(1.9684908663400869), np.float64(1.9677673416433707), np.float64(1.9670739664079573), np.float64(1.9664094768320979), np.float64(1.9657726623890588), np.float64(1.9651623635693893), np.float64(1.9645774697193532), np.float64(1.9640169169713961), np.float64(1.9634796862627166), np.float64(1.9629648014381782), np.float64(1.9624713274339431), np.float64(1.9619983685383888), np.float64(1.9615450667269971), np.float64(1.961110600068059), np.float64(1.9606941811961618), np.float64(1.9602950558505736), np.float64(1.959912501475745), np.float64(1.9595458258812772), np.float64(1.9591943659588202), np.float64(1.9588574864534687), np.float64(1.9585345787873298), np.float64(1.958225059933038), np.float64(1.957928371335084), np.float64(1.957643977876921), np.float64(1.9573713668918948), np.float64(1.9571100472161294), np.float64(1.956859548281576), np.float64(1.956619419247526), np.float64(1.9563892281689297), np.float64(1.9561685611999726), np.float64(1.9559570218313926), np.float64(1.9557542301601094), np.float64(1.955559822189788), np.float64(1.9553734491610208), np.float64(1.9551947769098632), np.float64(1.955023485253525), np.float64(1.9548592674020524), np.float64(1.9547018293949034), np.float64(1.9545508895613524), np.float64(1.9544061780037156), np.float64(1.9542674361024237), np.float64(1.954134416042013), np.float64(1.9540068803571522), np.float64(1.953884601497844), np.float64(1.9537673614129996), np.float64(1.9536549511515906), np.float64(1.953547170480649), np.float64(1.953443827519386), np.float64(1.953344738388755), np.float64(1.953249726875798), np.float64(1.953158624112151), np.float64(1.9530712682661102), np.float64(1.9529875042476776), np.float64(1.9529071834260407), np.float64(1.9528301633589622), np.float64(1.9527563075335646), np.float64(1.9526854851180402), np.float64(1.9526175707238134), np.float64(1.9525524441777176), np.float64(1.9524899903037645), np.float64(1.9524300987140912), np.float64(1.9523726636087115), np.float64(1.9523175835836812), np.float64(1.952264761447338), np.float64(1.9522141040442635), np.float64(1.9521655220866445), np.float64(1.9521189299927242), np.float64(1.9520742457320361), np.float64(1.9520313906771416), np.float64(1.951990289461594), np.float64(1.9519508698438617), np.float64(1.9519130625769678), np.float64(1.951876801283597), np.float64(1.9518420223364403), np.float64(1.9518086647435644), np.float64(1.9517766700385801), np.float64(1.9517459821754208), np.float64(1.9517165474275278), np.float64(1.951688314291262), np.float64(1.9516612333933583), np.float64(1.9516352574022595), np.float64(1.951610340943161), np.float64(1.9515864405166106), np.float64(1.951563514420518), np.float64(1.9515415226754207), np.float64(1.9515204269528863), np.float64(1.9515001905068972), np.float64(1.9514807781081136), np.float64(1.951462155980877), np.float64(1.9514442917428496), np.float64(1.9514271543471768), np.float64(1.9514107140270602), np.float64(1.9513949422426482), np.float64(1.951379811630146), np.float64(1.9513652959530414), np.float64(1.9513513700553764), np.float64(1.9513380098169573), np.float64(1.9513251921104406), np.float64(1.9513128947602043), np.float64(1.9513010965029316), np.float64(1.9512897769498427), np.float64(1.9512789165504953), np.float64(1.9512684965580935), np.float64(1.9512584989962423), np.float64(1.9512489066270846), np.float64(1.9512397029207666), np.float64(1.9512308720261706), np.float64(1.9512223987428658), np.float64(1.9512142684942284), np.float64(1.951206467301674), np.float64(1.9511989817599664), np.float64(1.9511917990135474), np.float64(1.9511849067338534), np.float64(1.951178293097573), np.float64(1.9511719467658057), np.float64(1.951165856864088), np.float64(1.9511600129632474), np.float64(1.9511544050610492), np.float64(1.9511490235646078), np.float64(1.951143859273524), np.float64(1.9511389033637219), np.float64(1.951134147371958), np.float64(1.9511295831809696), np.float64(1.9511252030052395), np.float64(1.9511209993773482), np.float64(1.9511169651348947), np.float64(1.9511130934079555), np.float64(1.9511093776070638), np.float64(1.951105811411685), np.float64(1.95110238875917), np.float64(1.9510991038341647), np.float64(1.9510959510584567), np.float64(1.9510929250812414), np.float64(1.9510900207697932), np.float64(1.9510872332005151), np.float64(1.9510845576503666), np.float64(1.9510819895886384), np.float64(1.9510795246690693), np.float64(1.9510771587222895), np.float64(1.9510748877485726), np.float64(1.9510727079108885), np.float64(1.9510706155282427), np.float64(1.951068607069288), np.float64(1.9510666791462035), np.float64(1.9510648285088195), np.float64(1.9510630520389909), np.float64(1.9510613467451985), np.float64(1.9510597097573716), np.float64(1.9510581383219265), np.float64(1.9510566297970091), np.float64(1.9510551816479276), np.float64(1.951053791442781), np.float64(1.9510524568482617), np.float64(1.9510511756256321), np.float64(1.9510499456268675), np.float64(1.9510487647909542), np.float64(1.9510476311403453), np.float64(1.9510465427775538), np.float64(1.9510454978818956), np.float64(1.9510444947063545), np.float64(1.9510435315745873), np.float64(1.951042606878043), np.float64(1.9510417190732043), np.float64(1.9510408666789403), np.float64(1.9510400482739703), np.float64(1.9510392624944268), np.float64(1.9510385080315236), np.float64(1.9510377836293162), np.float64(1.951037088082553), np.float64(1.9510364202346167), np.float64(1.95103577897555), np.float64(1.9510351632401597), np.float64(1.9510345720061995), np.float64(1.9510340042926286), np.float64(1.9510334591579377), np.float64(1.951032935698549), np.float64(1.9510324330472733), np.float64(1.9510319503718399), np.float64(1.9510314868734784), np.float64(1.951031041785562), np.float64(1.951030614372308), np.float64(1.9510302039275236), np.float64(1.9510298097734131), np.float64(1.9510294312594272), np.float64(1.9510290677611595), np.float64(1.9510287186792905), np.float64(1.9510283834385713), np.float64(1.9510280614868534), np.float64(1.9510277522941522), np.float64(1.951027455351754), np.float64(1.9510271701713549), np.float64(1.9510268962842374), np.float64(1.9510266332404806), np.float64(1.951026380608199), np.float64(1.9510261379728187), np.float64(1.9510259049363747), np.float64(1.9510256811168456), np.float64(1.9510254661475068), np.float64(1.9510252596763171), np.float64(1.951025061365325), np.float64(1.9510248708901026), np.float64(1.9510246879391993), np.float64(1.9510245122136214), np.float64(1.9510243434263295), np.float64(1.951024181301758), np.float64(1.9510240255753521), np.float64(1.9510238759931275), np.float64(1.9510237323112438), np.float64(1.9510235942955965), np.float64(1.951023461721427), np.float64(1.9510233343729455), np.float64(1.9510232120429736), np.float64(1.9510230945325953), np.float64(1.9510229816508267), np.float64(1.951022873214299), np.float64(1.9510227690469515), np.float64(1.9510226689797374), np.float64(1.9510225728503463), np.float64(1.95102248050293), np.float64(1.9510223917878469), np.float64(1.95102230656141), np.float64(1.951022224685652), np.float64(1.9510221460280934), np.float64(1.9510220704615246), np.float64(1.9510219978637937), np.float64(1.9510219281176053), np.float64(1.951021861110326), np.float64(1.9510217967337964), np.float64(1.9510217348841545), np.float64(1.951021675461663), np.float64(1.951021618370544), np.float64(1.951021563518821), np.float64(1.9510215108181685), np.float64(1.9510214601837643), np.float64(1.9510214115341504), np.float64(1.951021364791099), np.float64(1.9510213198794837), np.float64(1.9510212767271549), np.float64(1.9510212352648224), np.float64(1.9510211954259407), np.float64(1.951021157146599), np.float64(1.951021120365418), np.float64(1.951021085023447), np.float64(1.9510210510640695), np.float64(1.9510210184329069), np.float64(1.9510209870777335), np.float64(1.9510209569483874), np.float64(1.951020927996691), np.float64(1.9510209001763696), np.float64(1.9510208734429784), np.float64(1.9510208477538271), np.float64(1.9510208230679127), np.float64(1.9510207993458506), np.float64(1.9510207765498109), np.float64(1.9510207546434568), np.float64(1.9510207335918854), np.float64(1.9510207133615707), np.float64(1.951020693920309), np.float64(1.9510206752371655), np.float64(1.9510206572824256), np.float64(1.9510206400275465), np.float64(1.9510206234451086), np.float64(1.9510206075087726), np.float64(1.9510205921932375), np.float64(1.9510205774741975), np.float64(1.951020563328304), np.float64(1.951020549733127), np.float64(1.9510205366671194), np.float64(1.9510205241095815), np.float64(1.951020512040627), np.float64(1.9510205004411532), np.float64(1.951020489292806), np.float64(1.9510204785779546), np.float64(1.9510204682796597), np.float64(1.9510204583816477), np.float64(1.951020448868285), np.float64(1.9510204397245499), np.float64(1.951020430936012), np.float64(1.9510204224888068), np.float64(1.9510204143696135), np.float64(1.9510204065656347), np.float64(1.9510203990645734), np.float64(1.9510203918546156), np.float64(1.9510203849244112), np.float64(1.9510203782630535), np.float64(1.951020371860064), np.float64(1.9510203657053748), np.float64(1.9510203597893119), np.float64(1.9510203541025801), np.float64(1.951020348636248), np.float64(1.9510203433817346), np.float64(1.9510203383307934), np.float64(1.9510203334755025), np.float64(1.9510203288082473), np.float64(1.9510203243217124), np.float64(1.9510203200088692), np.float64(1.951020315862962), np.float64(1.9510203118775007), np.float64(1.9510203080462478), np.float64(1.9510203043632097), np.float64(1.9510203008226272), np.float64(1.9510202974189652), np.float64(1.9510202941469061), np.float64(1.9510202910013388), np.float64(1.9510202879773515), np.float64(1.9510202850702243), np.float64(1.9510202822754217), np.float64(1.951020279588585), np.float64(1.951020277005525), np.float64(1.9510202745222158), np.float64(1.9510202721347896), np.float64(1.9510202698395276), np.float64(1.951020267632857), np.float64(1.9510202655113436), np.float64(1.9510202634716873), np.float64(1.951020261510717), np.float64(1.9510202596253834), np.float64(1.9510202578127571), np.float64(1.9510202560700232), np.float64(1.9510202543944746), np.float64(1.9510202527835112), np.float64(1.9510202512346329), np.float64(1.9510202497454368), np.float64(1.9510202483136143), np.float64(1.9510202469369458), np.float64(1.9510202456132981), np.float64(1.9510202443406213), np.float64(1.9510202431169443), np.float64(1.9510202419403728), np.float64(1.9510202408090869), np.float64(1.9510202397213359), np.float64(1.9510202386754383), np.float64(1.9510202376697774), np.float64(1.9510202367027984), np.float64(1.9510202357730086), np.float64(1.9510202348789722), np.float64(1.951020234019308), np.float64(1.95102023319269), np.float64(1.9510202323978432), np.float64(1.9510202316335412), np.float64(1.9510202308986064), np.float64(1.9510202301919057), np.float64(1.9510202295123507), np.float64(1.9510202288588945), np.float64(1.9510202282305307), np.float64(1.9510202276262927), np.float64(1.95102022704525), np.float64(1.9510202264865097), np.float64(1.9510202259492122), np.float64(1.951020225432532), np.float64(1.9510202249356743), np.float64(1.9510202244578765), np.float64(1.951020223998404), np.float64(1.9510202235565526), np.float64(1.9510202231316431), np.float64(1.951020222723024), np.float64(1.9510202223300686), np.float64(1.9510202219521742), np.float64(1.951020221588761), np.float64(1.951020221239273), np.float64(1.9510202209031742), np.float64(1.9510202205799503), np.float64(1.951020220269106), np.float64(1.9510202199701658), np.float64(1.9510202196826714), np.float64(1.951020219406184), np.float64(1.9510202191402808), np.float64(1.9510202188845547), np.float64(1.9510202186386154), np.float64(1.9510202184020864), np.float64(1.9510202181746068), np.float64(1.9510202179558296), np.float64(1.9510202177454201), np.float64(1.9510202175430575), np.float64(1.9510202173484334), np.float64(1.9510202171612503), np.float64(1.9510202169812232), np.float64(1.9510202168080775), np.float64(1.9510202166415491), np.float64(1.9510202164813848), np.float64(1.95102021632734), np.float64(1.9510202161791808), np.float64(1.9510202160366807), np.float64(1.9510202158996237), np.float64(1.9510202157678012), np.float64(1.9510202156410124), np.float64(1.9510202155190643), np.float64(1.9510202154017717), np.float64(1.9510202152889564), np.float64(1.951020215180447), np.float64(1.9510202150760785), np.float64(1.9510202149756923), np.float64(1.9510202148791365), np.float64(1.951020214786264), np.float64(1.951020214696934), np.float64(1.951020214611011), np.float64(1.9510202145283648), np.float64(1.9510202144488697), np.float64(1.9510202143724056), np.float64(1.9510202142988566), np.float64(1.951020214228111), np.float64(1.9510202141600614), np.float64(1.9510202140946054), np.float64(1.9510202140316433), np.float64(1.9510202139710806), np.float64(1.9510202139128243), np.float64(1.9510202138567876), np.float64(1.9510202138028852), np.float64(1.951020213751035), np.float64(1.9510202137011596), np.float64(1.9510202136531836), np.float64(1.9510202136070334), np.float64(1.9510202135626402), np.float64(1.9510202135199368), np.float64(1.9510202134788586), np.float64(1.9510202134393435), np.float64(1.951020213401332), np.float64(1.9510202133647665), np.float64(1.9510202133295922), np.float64(1.951020213295756), np.float64(1.9510202132632062), np.float64(1.9510202132318946), np.float64(1.9510202132017738), np.float64(1.9510202131727983), np.float64(1.951020213144924), np.float64(1.9510202131181096), np.float64(1.9510202130923142), np.float64(1.9510202130674996), np.float64(1.9510202130436278), np.float64(1.9510202130206629), np.float64(1.9510202129985708), np.float64(1.9510202129773182), np.float64(1.9510202129568726), np.float64(1.9510202129372038), np.float64(1.9510202129182816), np.float64(1.9510202129000787), np.float64(1.9510202128825667), np.float64(1.9510202128657195), np.float64(1.951020212849512), np.float64(1.9510202128339198), np.float64(1.9510202128189196), np.float64(1.9510202128044887), np.float64(1.9510202127906053), np.float64(1.9510202127772487), np.float64(1.9510202127643992), np.float64(1.9510202127520369), np.float64(1.951020212740144), np.float64(1.9510202127287017), np.float64(1.9510202127176939), np.float64(1.9510202127071032), np.float64(1.9510202126969145), np.float64(1.9510202126871121), np.float64(1.9510202126776814), np.float64(1.9510202126686083), np.float64(1.951020212659879), np.float64(1.9510202126514806), np.float64(1.9510202126434006), np.float64(1.9510202126356269), np.float64(1.9510202126281477), np.float64(1.9510202126209522), np.float64(1.9510202126140288), np.float64(1.951020212607368), np.float64(1.9510202126009597), np.float64(1.951020212594794), np.float64(1.9510202125888618), np.float64(1.9510202125831544), np.float64(1.9510202125776632), np.float64(1.9510202125723795), np.float64(1.9510202125672964), np.float64(1.9510202125624059), np.float64(1.9510202125576999), np.float64(1.9510202125531724), np.float64(1.9510202125488165), np.float64(1.9510202125446252), np.float64(1.9510202125405924), np.float64(1.9510202125367127), np.float64(1.9510202125329794), np.float64(1.9510202125293874), np.float64(1.9510202125259315), np.float64(1.9510202125226064), np.float64(1.9510202125194067), np.float64(1.9510202125163283), np.float64(1.9510202125133667), np.float64(1.9510202125105165), np.float64(1.9510202125077742), np.float64(1.951020212505136), np.float64(1.951020212502597), np.float64(1.9510202125001546), np.float64(1.9510202124978042), np.float64(1.951020212495543), np.float64(1.9510202124933664), np.float64(1.951020212491273), np.float64(1.9510202124892584), np.float64(1.95102021248732), np.float64(1.9510202124854545), np.float64(1.9510202124836598), np.float64(1.9510202124819331), np.float64(1.9510202124802714), np.float64(1.9510202124786726), np.float64(1.9510202124771339), np.float64(1.9510202124756537), np.float64(1.9510202124742293), np.float64(1.9510202124728586), np.float64(1.9510202124715397), np.float64(1.951020212470271), np.float64(1.9510202124690497), np.float64(1.9510202124678746), np.float64(1.9510202124667437), np.float64(1.9510202124656557), np.float64(1.951020212464609), np.float64(1.9510202124636016), np.float64(1.9510202124626324), np.float64(1.9510202124616998), np.float64(1.951020212460802), np.float64(1.9510202124599383), np.float64(1.951020212459107), np.float64(1.9510202124583074), np.float64(1.951020212457538), np.float64(1.9510202124567975), np.float64(1.951020212456085), np.float64(1.9510202124553995), np.float64(1.9510202124547398), np.float64(1.9510202124541047), np.float64(1.9510202124534934), np.float64(1.951020212452906), np.float64(1.95102021245234), np.float64(1.9510202124517957), np.float64(1.9510202124512714), np.float64(1.9510202124507676), np.float64(1.9510202124502825), np.float64(1.9510202124498155), np.float64(1.9510202124493665), np.float64(1.9510202124489342), np.float64(1.951020212448518), np.float64(1.9510202124481177), np.float64(1.9510202124477327), np.float64(1.951020212447362), np.float64(1.9510202124470053), np.float64(1.9510202124466618), np.float64(1.9510202124463316), np.float64(1.9510202124460136), np.float64(1.9510202124457077), np.float64(1.9510202124454135), np.float64(1.9510202124451301), np.float64(1.9510202124448575), np.float64(1.951020212444595), np.float64(1.9510202124443428), np.float64(1.9510202124440998), np.float64(1.9510202124438658), np.float64(1.9510202124436409), np.float64(1.9510202124434244), np.float64(1.951020212443216), np.float64(1.9510202124430156), np.float64(1.9510202124428226), np.float64(1.9510202124426366), np.float64(1.9510202124424583), np.float64(1.9510202124422862), np.float64(1.9510202124421205), np.float64(1.9510202124419616), np.float64(1.9510202124418083), np.float64(1.9510202124416605), np.float64(1.9510202124415186), np.float64(1.951020212441382), np.float64(1.9510202124412506), np.float64(1.951020212441124), np.float64(1.9510202124410023), np.float64(1.9510202124408853), np.float64(1.9510202124407723), np.float64(1.9510202124406635), np.float64(1.9510202124405596), np.float64(1.9510202124404588), np.float64(1.9510202124403622), np.float64(1.951020212440269), np.float64(1.9510202124401794), np.float64(1.9510202124400933), np.float64(1.9510202124400104), np.float64(1.9510202124399303), np.float64(1.9510202124398535), np.float64(1.9510202124397797), np.float64(1.9510202124397087), np.float64(1.95102021243964), np.float64(1.9510202124395741), np.float64(1.9510202124395106), np.float64(1.9510202124394496), np.float64(1.9510202124393907), np.float64(1.9510202124393343), np.float64(1.9510202124392797), np.float64(1.9510202124392277), np.float64(1.9510202124391771), np.float64(1.9510202124391285), np.float64(1.951020212439082), np.float64(1.9510202124390368), np.float64(1.9510202124389937), np.float64(1.9510202124389522), np.float64(1.951020212438912), np.float64(1.9510202124388734), np.float64(1.9510202124388363), np.float64(1.9510202124388005), np.float64(1.9510202124387663), np.float64(1.9510202124387335), np.float64(1.9510202124387015), np.float64(1.9510202124386706), np.float64(1.9510202124386413), np.float64(1.951020212438613), np.float64(1.9510202124385856), np.float64(1.9510202124385592), np.float64(1.951020212438534), np.float64(1.9510202124385096), np.float64(1.9510202124384863), np.float64(1.9510202124384637), np.float64(1.9510202124384421), np.float64(1.9510202124384213), np.float64(1.9510202124384008), np.float64(1.9510202124383818), np.float64(1.9510202124383629), np.float64(1.951020212438345), np.float64(1.951020212438328), np.float64(1.9510202124383114), np.float64(1.9510202124382952), np.float64(1.95102021243828), np.float64(1.9510202124382652), np.float64(1.9510202124382507), np.float64(1.9510202124382372), np.float64(1.9510202124382239), np.float64(1.9510202124382112), np.float64(1.951020212438199), np.float64(1.9510202124381872), np.float64(1.951020212438176), np.float64(1.951020212438165), np.float64(1.9510202124381548), np.float64(1.9510202124381446), np.float64(1.9510202124381348), np.float64(1.9510202124381255), np.float64(1.9510202124381166), np.float64(1.951020212438108), np.float64(1.9510202124380995), np.float64(1.9510202124380913), np.float64(1.9510202124380838), np.float64(1.9510202124380762), np.float64(1.9510202124380693), np.float64(1.9510202124380622), np.float64(1.9510202124380558), np.float64(1.9510202124380496), np.float64(1.9510202124380434), np.float64(1.9510202124380371), np.float64(1.9510202124380316), np.float64(1.9510202124380258), np.float64(1.9510202124380207), np.float64(1.9510202124380154), np.float64(1.9510202124380107), np.float64(1.951020212438006), np.float64(1.9510202124380016), np.float64(1.9510202124379972), np.float64(1.9510202124379932), np.float64(1.9510202124379892), np.float64(1.9510202124379852), np.float64(1.9510202124379814), np.float64(1.951020212437978), np.float64(1.9510202124379743), np.float64(1.9510202124379712), np.float64(1.9510202124379679), np.float64(1.9510202124379645), np.float64(1.9510202124379616), np.float64(1.951020212437959), np.float64(1.9510202124379563), np.float64(1.9510202124379536), np.float64(1.951020212437951), np.float64(1.9510202124379488), np.float64(1.9510202124379463), np.float64(1.951020212437944), np.float64(1.9510202124379419), np.float64(1.9510202124379394), np.float64(1.9510202124379377), np.float64(1.9510202124379354), np.float64(1.9510202124379334), np.float64(1.951020212437932), np.float64(1.9510202124379303), np.float64(1.9510202124379283), np.float64(1.951020212437927), np.float64(1.9510202124379254), np.float64(1.951020212437924), np.float64(1.9510202124379223), np.float64(1.951020212437921), np.float64(1.95102021243792), np.float64(1.9510202124379186), np.float64(1.9510202124379172), np.float64(1.9510202124379161), np.float64(1.9510202124379148), np.float64(1.9510202124379137), np.float64(1.9510202124379126), np.float64(1.9510202124379117), np.float64(1.9510202124379108), np.float64(1.9510202124379097), np.float64(1.9510202124379088), np.float64(1.951020212437908), np.float64(1.9510202124379072), np.float64(1.9510202124379064), np.float64(1.9510202124379057), np.float64(1.9510202124379048), np.float64(1.9510202124379041), np.float64(1.9510202124379037), np.float64(1.9510202124379028), np.float64(1.9510202124379021), np.float64(1.9510202124379017), np.float64(1.9510202124379008), np.float64(1.9510202124379004), np.float64(1.9510202124379), np.float64(1.9510202124378995), np.float64(1.9510202124378986), np.float64(1.9510202124378981), np.float64(1.9510202124378977), np.float64(1.9510202124378972), np.float64(1.9510202124378968), np.float64(1.9510202124378966), np.float64(1.9510202124378961), np.float64(1.9510202124378955), np.float64(1.9510202124378953), np.float64(1.951020212437895), np.float64(1.9510202124378946), np.float64(1.9510202124378946), np.float64(1.9510202124378941), np.float64(1.9510202124378935), np.float64(1.9510202124378933), np.float64(1.9510202124378933), np.float64(1.9510202124378928), np.float64(1.9510202124378926), np.float64(1.9510202124378921), np.float64(1.951020212437892), np.float64(1.9510202124378917), np.float64(1.9510202124378915), np.float64(1.9510202124378915), np.float64(1.951020212437891), np.float64(1.9510202124378908), np.float64(1.9510202124378906), np.float64(1.9510202124378906), np.float64(1.9510202124378906), np.float64(1.9510202124378897), np.float64(1.9510202124378897), np.float64(1.95102021243789), np.float64(1.9510202124378895), np.float64(1.9510202124378897), np.float64(1.9510202124378893), np.float64(1.9510202124378893), np.float64(1.9510202124378893), np.float64(1.9510202124378888), np.float64(1.9510202124378888), np.float64(1.9510202124378886), np.float64(1.9510202124378884), np.float64(1.9510202124378884), np.float64(1.9510202124378884), np.float64(1.9510202124378884), np.float64(1.9510202124378884), np.float64(1.9510202124378881), np.float64(1.951020212437888), np.float64(1.951020212437888), np.float64(1.951020212437888), np.float64(1.9510202124378877), np.float64(1.951020212437888), np.float64(1.9510202124378873), np.float64(1.9510202124378875), np.float64(1.9510202124378873), np.float64(1.9510202124378875), np.float64(1.9510202124378875), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.951020212437887), np.float64(1.9510202124378868), np.float64(1.9510202124378866), np.float64(1.9510202124378868), np.float64(1.9510202124378866), np.float64(1.9510202124378868), np.float64(1.9510202124378868), np.float64(1.9510202124378866), np.float64(1.9510202124378868), np.float64(1.9510202124378864), np.float64(1.9510202124378864), np.float64(1.9510202124378861), np.float64(1.9510202124378864), np.float64(1.9510202124378864), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378864), np.float64(1.9510202124378864), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378864), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.951020212437886), np.float64(1.9510202124378861), np.float64(1.9510202124378861), np.float64(1.9510202124378857), np.float64(1.9510202124378861), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.9510202124378857), np.float64(1.951020212437886), np.float64(1.951020212437886), np.float64(1.9510202124378857), np.float64(1.9510202124378857), np.float64(1.9510202124378857), np.float64(1.9510202124378861), np.float64(1.9510202124378857), np.float64(1.9510202124378857), np.float64(1.951020212437886), np.float64(1.9510202124378857)])\n"
     ]
    }
   ],
   "source": [
    "w_rigde, b_rigde, losses = train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")\n",
    "\n",
    "print(train_ridge_regression(X, y, learning_rate=0.01, reg_lambda=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
